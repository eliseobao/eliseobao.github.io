<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>demonstrating cabuxa-7b | Eliseo Bao</title> <meta name="author" content="Eliseo Bao"> <meta name="description" content="how to use an llm with galician-speaking ability"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://iosevka-webfonts.github.io/iosevka/iosevka.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://eliseobao.github.io/blog/2023/cabuxa/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">EliseoÂ </span>Bao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">demonstrating cabuxa-7b</h1> <p class="post-meta">December 12, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> Â  Â· Â  <a href="/blog/tag/jupyter"> <i class="fas fa-hashtag fa-sm"></i> jupyter</a> Â  <a href="/blog/tag/galician"> <i class="fas fa-hashtag fa-sm"></i> galician</a> Â  <a href="/blog/tag/low-resources"> <i class="fas fa-hashtag fa-sm"></i> low-resources</a> Â  <a href="/blog/tag/multilingual"> <i class="fas fa-hashtag fa-sm"></i> multilingual</a> Â  <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> llm</a> Â  Â  Â· Â  <a href="/blog/category/tutorials"> <i class="fas fa-tag fa-sm"></i> tutorials</a> Â  </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="introduction">Introduction</h3> <p>Hi! In this (my first ever ðŸ˜…) blog post Iâ€™m going to show you how to run <a href="https://huggingface.co/irlab-udc/cabuxa-7b" rel="external nofollow noopener" target="_blank">Cabuxa-7B</a>. I recommend you take a look at the working notes (see references below). But, if you want a very quick intro, let me tell you that Cabuxa-7B is an LLaMA-7B LoRA-instruct-tuned model for Galician that can answer instructions in the <a href="https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json" rel="external nofollow noopener" target="_blank">Alpaca format</a>. Galician is my native language and, like many other low-resource languages in the world, it is underrepresented in the current (impressive) landscape of AI technologies. So, this is our humble attempt to contribute to the inclusion of all linguistic communities in the development of Large Language Models (LLMs).</p> <h3 id="setting-up-the-environment">Setting Up the Environment</h3> <p>Before jumping into the demonstration, ensure you have the required dependencies installed by executing the following command:</p> <div class="language-zsh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">transformers</span><span class="o">==</span>4.34.0 <span class="o">&amp;&amp;</span> pip <span class="nb">install </span><span class="nv">peft</span><span class="o">==</span>0.6.0 <span class="o">&amp;&amp;</span> pip <span class="nb">install </span><span class="nv">sentencepiece</span><span class="o">==</span>0.1.99
</code></pre></div></div> <p>Now, letâ€™s import the necessary components:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">PeftConfig</span><span class="p">,</span> <span class="n">PeftModel</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">LlamaTokenizer</span><span class="p">,</span> <span class="n">GenerationConfig</span>
</code></pre></div></div> <h3 id="loading-the-model-and-tokenizer">Loading the Model and Tokenizer</h3> <p>Low-Rank Adaptation (LoRA) is a technique that can be used to improve the efficiency of fine-tuning LLMs by using only a subset of the model weights, resulting in a significant reduction in memory requirements. In this way, Cabuxa-7B works as an adapter on top of LLaMA (its base model).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">irlab-udc/cabuxa-7b</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">huggyllama/llama-7b</span><span class="sh">"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">"</span><span class="s">irlab-udc/cabuxa-7b</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">LlamaTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">huggyllama/llama-7b</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="prompt-generation-and-evaluation">Prompt Generation and Evaluation</h3> <p>We define two functions: Â´generate_promptÂ´ for creating prompts with or without additional input and Â´evaluateÂ´ for generating responses. The Â´evaluateÂ´ function takes an instruction and optional input, generates a prompt, and prints the modelâ€™s response.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_prompt</span><span class="p">(</span><span class="n">instruction</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">input</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Abaixo estÃ¡ unha instruciÃ³n que describe unha tarefa, xunto cunha entrada que proporciona mÃ¡is contexto. 
               Escribe unha resposta que responda adecuadamente a entrada.
               ### InstruciÃ³n:
               </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s">
               ### Entrada:
               </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s">
               ### Resposta:</span><span class="sh">"""</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Abaixo estÃ¡ unha instruciÃ³n que describe unha tarefa.
               Escribe unha resposta que responda adecuadamente a entrada.
               ### InstruciÃ³n:
               </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s">
               ### Resposta:</span><span class="sh">"""</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">instruction</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nf">generate_prompt</span><span class="p">(</span><span class="n">instruction</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">].</span><span class="nf">cuda</span><span class="p">()</span>
    <span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="nc">GenerationConfig</span><span class="p">(</span><span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">generation_output</span><span class="p">.</span><span class="n">sequences</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Resposta:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">### Resposta:</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">strip</span><span class="p">())</span>
</code></pre></div></div> <h3 id="example-usages">Example Usages</h3> <p>And finally, letâ€™s see Cabuxa-7B in action with a couple of examples. Outputs are not perfect, but there exists room for improvement as instruct-tuning data was very limited.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">evaluate</span><span class="p">(</span><span class="sh">"</span><span class="s">Cal Ã© a fÃ³rmula quÃ­mica da auga?</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Resposta</span><span class="p">:</span> <span class="n">A</span> <span class="n">fÃ³rmula</span> <span class="n">quÃ­mica</span> <span class="n">da</span> <span class="n">auga</span> <span class="n">Ã©</span> <span class="n">H</span><span class="err">â‚‚</span><span class="n">O</span><span class="p">.</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">evaluate</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">Convence ao lector por que Ã© importante un determinado tema.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Por que Ã© esencial priorizar o sono?</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Resposta</span><span class="p">:</span> <span class="n">Comezar</span> <span class="n">co</span> <span class="n">dormir</span> <span class="n">corrixidamente</span> <span class="n">asegura</span> <span class="n">unha</span> <span class="n">boa</span> <span class="n">almacenaxe</span> <span class="n">de</span> <span class="n">enerxÃ­a</span><span class="p">,</span> <span class="n">mellora</span> <span class="n">o</span> <span class="n">proceso</span> <span class="n">de</span> <span class="n">aprendizaxe</span> <span class="n">e</span> <span class="n">aumenta</span> <span class="n">a</span> <span class="n">lonxitude</span> <span class="n">de</span> <span class="n">sangra</span><span class="p">.</span> <span class="n">Desgraciadamente</span><span class="p">,</span> <span class="n">a</span> <span class="n">maiorÃ­a</span> <span class="n">das</span> <span class="n">persoas</span> <span class="n">non</span> <span class="n">prioriza</span> <span class="n">o</span> <span class="n">seu</span> <span class="n">sono</span> <span class="n">e</span><span class="p">,</span> <span class="n">consecuentemente</span><span class="p">,</span> <span class="n">eles</span> <span class="n">non</span> <span class="n">disfrutan</span> <span class="n">o</span> <span class="n">maior</span> <span class="n">beneficio</span> <span class="n">potencial</span><span class="p">.</span> <span class="n">SerÃ­a</span> <span class="n">importante</span> <span class="n">xestionar</span> <span class="n">un</span> <span class="n">haupaxeo</span> <span class="n">de</span> <span class="n">tiempo</span> <span class="n">para</span> <span class="n">dormir</span> <span class="n">para</span> <span class="n">aumentar</span> <span class="n">a</span> <span class="n">xestion</span> <span class="n">de</span> <span class="n">enerxÃ­a</span> <span class="n">e</span> <span class="n">a</span> <span class="n">eficiencia</span><span class="p">,</span> <span class="n">aforrar</span> <span class="n">tempo</span> <span class="n">para</span> <span class="n">aprender</span> <span class="n">e</span> <span class="n">disfrutar</span> <span class="n">do</span> <span class="n">son</span><span class="p">,</span> <span class="n">asÃ­</span> <span class="n">como</span> <span class="n">aforrar</span> <span class="n">eficiencia</span> <span class="n">fÃ­sica</span> <span class="n">e</span> <span class="n">mental</span><span class="p">.</span> <span class="n">DevolverÃ¡</span> <span class="n">resultados</span> <span class="n">positivos</span> <span class="n">ao</span> <span class="n">teu</span> <span class="n">organismo</span> <span class="n">e</span><span class="p">,</span> <span class="n">en</span> <span class="n">consecuencia</span><span class="p">,</span> <span class="n">ao</span> <span class="n">teu</span> <span class="n">benestar</span><span class="err">!</span>
</code></pre></div></div> <h3 id="try-it-yourself">Try It Yourself!</h3> <p>For an interactive experience, you can access the Google Colab notebook <a href="https://colab.research.google.com/drive/1wH2cT7gbUDlcwNOA4OcNs0ptLv99njkA?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>. Remember to set GPU as hardware accelerator on runtime options.</p> <p>Feel free to explore and experiment with Cabuxa-7B, and donâ€™t hesitate to provide feedback or suggestions for improvement. See you!</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#b31b1b"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div> <div id="bao2023conversations" class="col-sm-8"> <div class="title">Conversations in Galician: a Large Language Model for an Underrepresented Language</div> <div class="author"> <em>Eliseo Bao</em>,Â Anxo PÃ©rez,Â andÂ Javier Parapar</div> <div class="periodical"> <em>arXiv preprint</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.03812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/irlab-udc/cabuxa-7b" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2311.03812.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gitlab.irlab.org/irlab/cabuxa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The recent proliferation of Large Conversation Language Models has highlighted the economic significance of widespread access to this type of AI technologies in the current information age. Nevertheless, prevailing models have primarily been trained on corpora consisting of documents written in popular languages. The dearth of such cutting-edge tools for low-resource languages further exacerbates their underrepresentation in the current economic landscape, thereby impacting their native speakers. This paper introduces two novel resources designed to enhance Natural Language Processing (NLP) for the Galician language. We present a Galician adaptation of the Alpaca dataset, comprising 52,000 instructions and demonstrations. This dataset proves invaluable for enhancing language models by fine-tuning them to more accurately adhere to provided instructions. Additionally, as a demonstration of the dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician, a language not originally supported by the model, by following the Alpaca format. This work contributes to the research on multilingual models tailored for low-resource settings, a crucial endeavor in ensuring the inclusion of all linguistic communities in the development of Large Language Models. Another noteworthy aspect of this research is the exploration of how knowledge of a closely related language, in this case, Portuguese, can assist in generating coherent text when training resources are scarce. Both the Galician Alpaca dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we have made the source code available to facilitate replication of this experiment and encourage further advancements for underrepresented languages.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bao2023conversations</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Conversations in Galician: a Large Language Model for an Underrepresented Language}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bao, Eliseo and PÃ©rez, Anxo and Parapar, Javier}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.03812}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"eliseobao/eliseobao.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2023 Eliseo Bao. Last updated: December 13, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>