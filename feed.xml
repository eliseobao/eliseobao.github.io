<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://eliseobao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://eliseobao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-04T16:39:27+00:00</updated><id>https://eliseobao.github.io/feed.xml</id><title type="html">blank</title><subtitle>My personal website. Still under construction. </subtitle><entry><title type="html">demonstrating cabuxa-7b</title><link href="https://eliseobao.github.io/blog/2023/cabuxa/" rel="alternate" type="text/html" title="demonstrating cabuxa-7b"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://eliseobao.github.io/blog/2023/cabuxa</id><content type="html" xml:base="https://eliseobao.github.io/blog/2023/cabuxa/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Hi! In this (my first ever 😅) blog post I’m going to show you how to run <a href="https://huggingface.co/irlab-udc/cabuxa-7b">Cabuxa-7B</a>. I recommend you take a look at the working notes (see references below). But, if you want a very quick intro, let me tell you that Cabuxa-7B is an LLaMA-7B LoRA-instruct-tuned model for Galician that can answer instructions in the <a href="https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json">Alpaca format</a>. Galician is my native language and, like many other low-resource languages in the world, it is underrepresented in the current (impressive) landscape of AI technologies. So, this is a humble attempt to contribute to the inclusion of all linguistic communities in the development of Large Language Models (LLMs).</p> <h3 id="setting-up-the-environment">Setting Up the Environment</h3> <p>Before jumping into the demonstration, ensure you have the required dependencies installed by executing the following command:</p> <div class="language-zsh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">transformers</span><span class="o">==</span>4.34.0 <span class="o">&amp;&amp;</span> pip <span class="nb">install </span><span class="nv">peft</span><span class="o">==</span>0.6.0 <span class="o">&amp;&amp;</span> pip <span class="nb">install </span><span class="nv">sentencepiece</span><span class="o">==</span>0.1.99
</code></pre></div></div> <p>Now, let’s import the necessary components:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">PeftConfig</span><span class="p">,</span> <span class="n">PeftModel</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">LlamaTokenizer</span><span class="p">,</span> <span class="n">GenerationConfig</span>
</code></pre></div></div> <h3 id="loading-the-model-and-tokenizer">Loading the Model and Tokenizer</h3> <p>Low-Rank Adaptation (LoRA) is a technique that can be used to improve the efficiency of fine-tuning LLMs by using only a subset of the model weights, resulting in a significant reduction in memory requirements. In this way, Cabuxa-7B works as an adapter on top of LLaMA (its base model).</p> <p>About 7 * 4 = 28GB of GPU memory are required to run the 7B model in full precision. In order to use half the memory and fit the model on a T4 (see link to Google Colab below), we set <code class="language-plaintext highlighter-rouge">torch_dtype=torch.float16</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">irlab-udc/cabuxa-7b</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">huggyllama/llama-7b</span><span class="sh">"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">"</span><span class="s">irlab-udc/cabuxa-7b</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">LlamaTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">huggyllama/llama-7b</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="prompt-generation-and-evaluation">Prompt Generation and Evaluation</h3> <p>We define two functions: <code class="language-plaintext highlighter-rouge">generate_prompt</code> for creating prompts with or without additional input and <code class="language-plaintext highlighter-rouge">evaluate</code> for generating responses. The <code class="language-plaintext highlighter-rouge">evaluate</code> function takes an instruction and optional input, generates a prompt, and prints the model’s response.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_prompt</span><span class="p">(</span><span class="n">instruction</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">input</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Abaixo está unha instrución que describe unha tarefa, xunto cunha entrada que proporciona máis contexto. 
               Escribe unha resposta que responda adecuadamente a entrada.
               ### Instrución:
               </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s">
               ### Entrada:
               </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s">
               ### Resposta:</span><span class="sh">"""</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Abaixo está unha instrución que describe unha tarefa.
               Escribe unha resposta que responda adecuadamente a entrada.
               ### Instrución:
               </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s">
               ### Resposta:</span><span class="sh">"""</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">instruction</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nf">generate_prompt</span><span class="p">(</span><span class="n">instruction</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">].</span><span class="nf">cuda</span><span class="p">()</span>
    <span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="nc">GenerationConfig</span><span class="p">(</span><span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">generation_output</span><span class="p">.</span><span class="n">sequences</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Resposta:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">### Resposta:</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">strip</span><span class="p">())</span>
</code></pre></div></div> <h3 id="example-usages">Example Usages</h3> <p>And finally, let’s see Cabuxa-7B in action with a couple of examples. Outputs are not perfect, but there exists room for improvement as instruct-tuning data was very limited.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">evaluate</span><span class="p">(</span><span class="sh">"</span><span class="s">Cal é a fórmula química da auga?</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Resposta</span><span class="p">:</span> <span class="n">A</span> <span class="n">fórmula</span> <span class="n">química</span> <span class="n">da</span> <span class="n">auga</span> <span class="n">é</span> <span class="n">H2O</span><span class="p">.</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">evaluate</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">Convence ao lector por que é importante un determinado tema.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Por que é esencial priorizar o sono?</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Resposta</span><span class="p">:</span> <span class="n">Comezar</span> <span class="n">co</span> <span class="n">dormir</span> <span class="n">corrixidamente</span> <span class="n">asegura</span> <span class="n">unha</span> <span class="n">boa</span> <span class="n">almacenaxe</span> <span class="n">de</span> <span class="n">enerxía</span><span class="p">,</span> <span class="n">mellora</span> <span class="n">o</span> <span class="n">proceso</span> <span class="n">de</span> <span class="n">aprendizaxe</span> <span class="n">e</span> <span class="n">aumenta</span> <span class="n">a</span> <span class="n">lonxitude</span> <span class="n">de</span> <span class="n">sangra</span><span class="p">.</span> <span class="n">Desgraciadamente</span><span class="p">,</span> <span class="n">a</span> <span class="n">maioría</span> <span class="n">das</span> <span class="n">persoas</span> <span class="n">non</span> <span class="n">prioriza</span> <span class="n">o</span> <span class="n">seu</span> <span class="n">sono</span> <span class="n">e</span><span class="p">,</span> <span class="n">consecuentemente</span><span class="p">,</span> <span class="n">eles</span> <span class="n">non</span> <span class="n">disfrutan</span> <span class="n">o</span> <span class="n">maior</span> <span class="n">beneficio</span> <span class="n">potencial</span><span class="p">.</span> <span class="n">Sería</span> <span class="n">importante</span> <span class="n">xestionar</span> <span class="n">un</span> <span class="n">haupaxeo</span> <span class="n">de</span> <span class="n">tiempo</span> <span class="n">para</span> <span class="n">dormir</span> <span class="n">para</span> <span class="n">aumentar</span> <span class="n">a</span> <span class="n">xestion</span> <span class="n">de</span> <span class="n">enerxía</span> <span class="n">e</span> <span class="n">a</span> <span class="n">eficiencia</span><span class="p">,</span> <span class="n">aforrar</span> <span class="n">tempo</span> <span class="n">para</span> <span class="n">aprender</span> <span class="n">e</span> <span class="n">disfrutar</span> <span class="n">do</span> <span class="n">son</span><span class="p">,</span> <span class="n">así</span> <span class="n">como</span> <span class="n">aforrar</span> <span class="n">eficiencia</span> <span class="n">física</span> <span class="n">e</span> <span class="n">mental</span><span class="p">.</span> <span class="n">Devolverá</span> <span class="n">resultados</span> <span class="n">positivos</span> <span class="n">ao</span> <span class="n">teu</span> <span class="n">organismo</span> <span class="n">e</span><span class="p">,</span> <span class="n">en</span> <span class="n">consecuencia</span><span class="p">,</span> <span class="n">ao</span> <span class="n">teu</span> <span class="n">benestar</span><span class="err">!</span>
</code></pre></div></div> <h3 id="try-it-yourself">Try It Yourself!</h3> <p>For an interactive experience, you can access the <strong>Google Colab notebook</strong> <a href="https://colab.research.google.com/drive/1wH2cT7gbUDlcwNOA4OcNs0ptLv99njkA?usp=sharing">here</a>. Remember to set GPU as hardware accelerator on runtime options.</p> <p>Feel free to explore and experiment with Cabuxa-7B, and don’t hesitate to provide feedback or suggestions for improvement. See you!</p>]]></content><author><name></name></author><category term="tutorials"/><category term="jupyter"/><category term="galician"/><category term="low-resources"/><category term="multilingual"/><category term="llm"/><summary type="html"><![CDATA[how to use an llm with galician-speaking ability]]></summary></entry></feed>